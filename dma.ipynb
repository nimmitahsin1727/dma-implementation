{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMA - Scratch Implementation\n",
    "\n",
    "Here, I'm implementing the DMA model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UcOPoLTScu4V"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMA:\n",
    "    def __init__(self, K, alpha, beta):\n",
    "        self.K = K  # number of topics\n",
    "        self.alpha = alpha  # hyperparameter for the document-topic distribution\n",
    "        self.beta = beta  # hyperparameter for the topic-word distribution\n",
    "\n",
    "    def fit(self, X, max_iter=100):\n",
    "        N, V = X.shape\n",
    "        z = np.zeros((N, V), dtype=int)  # topic assignments for each word in each document\n",
    "        n_z = np.zeros(self.K, dtype=int)  # number of words assigned to each topic\n",
    "        n_zw = np.zeros((self.K, V), dtype=int)  # number of occurrences of each word in each topic\n",
    "        n_zd = np.zeros((self.K, N), dtype=int)  # number of words assigned to each topic in each document\n",
    "        n_z_sum = np.zeros(self.K, dtype=int)  # total number of words assigned to each topic\n",
    "\n",
    "        # Initialize topic assignments randomly\n",
    "        for i in range(N):\n",
    "            for j in range(V):\n",
    "                if X[i, j] > 0:\n",
    "                    z[i, j] = np.random.choice(self.K)\n",
    "                    n_z[z[i, j]] += X[i, j]\n",
    "                    n_zw[z[i, j], j] += X[i, j]\n",
    "                    n_zd[z[i, j], i] += X[i, j]\n",
    "                    n_z_sum[z[i, j]] += X[i, j]\n",
    "\n",
    "        # Iterate over the data and update topic assignments\n",
    "        for _ in range(max_iter):\n",
    "            for i in range(N):\n",
    "                for j in range(V):\n",
    "                    if X[i, j] > 0:\n",
    "                        # Remove the current word from the topic assignment\n",
    "                        n_z[z[i, j]] -= X[i, j]\n",
    "                        n_zw[z[i, j], j] -= X[i, j]\n",
    "                        n_zd[z[i, j], i] -= X[i, j]\n",
    "                        n_z_sum[z[i, j]] -= X[i, j]\n",
    "\n",
    "                        # Compute the posterior probabilities of the topics\n",
    "                        p_z = (n_zw[:, j] + self.beta) / (n_z_sum + self.beta * V) * (n_zd[:, i] + self.alpha)\n",
    "                        p_z /= p_z.sum()\n",
    "\n",
    "                        # Sample a new topic assignment from the posterior probabilities\n",
    "                        z[i, j] = np.random.choice(self.K, p=p_z)\n",
    "\n",
    "                        # Add the current word to the new topic assignment\n",
    "                        n_z[z[i, j]] += X[i, j]\n",
    "                        n_zw[z[i, j], j] += X[i, j]\n",
    "                        n_zd[z[i, j], i] += X[i, j]\n",
    "                        n_z_sum[z[i, j]] += X[i, j]\n",
    "\n",
    "        # Compute the topic-word distribution\n",
    "        self.phi = (n_zw + self.beta) / (n_z_sum[:, np.newaxis] + self.beta * V)\n",
    "\n",
    "    def predict(self, X):\n",
    "        N, V = X.shape\n",
    "        p_z = np.zeros((N, self.K))\n",
    "\n",
    "        # Compute the posterior probabilities of the topics for each document\n",
    "        for i in range(N):\n",
    "            for j in range(V):\n",
    "                if X[i, j] > 0:\n",
    "                    p_z[i] += X[i, j] * np.log(self.phi[:, j])\n",
    "\n",
    "        # Add the prior on the document-topic distribution\n",
    "        p_z += np.log(self.alpha)\n",
    "\n",
    "        # Normalize the probabilities for each document\n",
    "        p_z = np.exp(p_z - p_z.max(axis=1, keepdims=True))\n",
    "        p_z /= p_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return p_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "N = 100  # number of documents\n",
    "V = 50  # vocabulary size\n",
    "K = 5  # number of topics\n",
    "\n",
    "# Generate document-topic distribution\n",
    "theta = np.random.dirichlet([0.5] * K, N)\n",
    "\n",
    "# Generate topic-word distribution\n",
    "phi = np.random.dirichlet([0.5] * V, K)\n",
    "\n",
    "# Generate documents\n",
    "X = np.zeros((N, V), dtype=int)\n",
    "for i in range(N):\n",
    "    z = np.random.choice(K, p=theta[i])\n",
    "    X[i] = np.random.multinomial(100, phi[z])\n",
    "\n",
    "# Fit DMA model\n",
    "alpha = 0.5\n",
    "beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dma = DMA(K, alpha, beta)\n",
    "dma.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new document\n",
    "X_new = np.zeros((5, V), dtype=int)\n",
    "for i in range(5):\n",
    "    z = np.random.choice(K, p=theta[i])\n",
    "    X_new[i] = np.random.multinomial(100, phi[z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the topic distribution for the new document\n",
    "predicted_topics = dma.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 - Predicted Topic Distribution: [1.32269992e-175 1.56879766e-147 1.00000000e+000 2.65316137e-135\n",
      " 1.48498789e-115]\n",
      "Document 2 - Predicted Topic Distribution: [4.88275435e-078 1.83287752e-078 3.98570932e-086 4.70088113e-106\n",
      " 1.00000000e+000]\n",
      "Document 3 - Predicted Topic Distribution: [3.21519914e-169 2.17509076e-124 1.00000000e+000 9.11490836e-149\n",
      " 1.18314102e-091]\n",
      "Document 4 - Predicted Topic Distribution: [1.06264695e-085 3.47572747e-089 1.82573773e-085 1.16590233e-101\n",
      " 1.00000000e+000]\n",
      "Document 5 - Predicted Topic Distribution: [1.89812934e-151 1.09980934e-127 1.00000000e+000 8.97912522e-145\n",
      " 1.35943884e-089]\n"
     ]
    }
   ],
   "source": [
    "# Print the predicted topic distribution for each document\n",
    "for i in range(5):\n",
    "    print(f\"Document {i+1} - Predicted Topic Distribution: {predicted_topics[i]}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UcOPoLTScu4V",
    "S_BBNjjzc4m5",
    "c0cAeBowGUVP"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lda-implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1669881b8e0ee381f1d44208a6e6b4675430ed382f288976bd9acdbb8db18405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
